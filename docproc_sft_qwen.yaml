# examples/docproc_sft_qwen.yaml
# Adjust to your LLaMA-Factory version if fields differ.
stage: sft
model_name_or_path: Qwen/Qwen2.5-7B

# Training data (SFT JSONL), generated by step1_process_data.py
datasets:
  - path: data/cloud_share/mcp_tools/doc_process/train_model/v1.jsonl
    type: sft

output_dir: outputs/qwen2_5_7b_docproc_sft
finetuning_type: lora

# Common hyperparameters (adjust as needed)
learning_rate: 2e-5
num_train_epochs: 3
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
save_steps: 200
logging_steps: 10
bf16: true

# Prompt field mapping can be configured via CLI/UI depending on version
# template: default
# sft_field: ["instruction", "input", "output"]
